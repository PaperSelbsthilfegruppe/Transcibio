{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9551b3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afi89\\Documents\\Github\\Transcibio-1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading diarization pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization pipeline loaded.\n",
      "Performing speaker diarization...\n",
      "Diarization complete.\n",
      "Found 40 speaker turns.\n",
      "Loading Whisper model 'tiny'...\n",
      "Whisper model loaded.\n",
      "Transcribing audio with word timestamps...\n",
      "Transcription complete.\n",
      "Aligning transcription with speaker segments...\n",
      "Alignment complete.\n",
      "\n",
      "--- Speaker-Aligned Transcript ---\n",
      "[0.00s - 5.16s] UNKNOWN_SPEAKER: Die Wieddeutschlernen\n",
      "[5.16s - 7.06s] SPEAKER_00: mit dem Deutsch-Treiner.\n",
      "[35.02s - 36.34s] UNKNOWN_SPEAKER: Thanks,\n",
      "[36.98s - 38.70s] SPEAKER_00: many thanks. Danke\n",
      "[38.70s - 39.40s] UNKNOWN_SPEAKER: für\n",
      "[39.40s - 39.56s] SPEAKER_00: den\n",
      "[39.56s - 51.96s] UNKNOWN_SPEAKER: Dank. Du bist ein Wieddeutschlernen mit dem Deutsch-Treiner. Mit\n",
      "[51.96s - 60.64s] SPEAKER_00: Plecher. Ganna. Anytime. Ganna-Geschin.\n",
      "[64.02s - 65.42s] UNKNOWN_SPEAKER: No\n",
      "[65.42s - 82.84s] SPEAKER_00: problem. Kein Problem. It doesn't matter. Das macht nicht. Congratulations. Herzlichen Glückwunsch.\n",
      "[82.84s - 87.84s] UNKNOWN_SPEAKER: Guten Tag.\n",
      "[89.08s - 89.64s] SPEAKER_00: Vielen Glück.\n",
      "[93.50s - 95.82s] UNKNOWN_SPEAKER: Ich freue\n",
      "[95.82s - 96.32s] SPEAKER_00: mich.\n",
      "[99.62s - 108.50s] UNKNOWN_SPEAKER: Das ist teuer. Korschen.\n",
      "[108.80s - 124.64s] SPEAKER_00: Ach du. No way. Auf keinen Fall. Unfortunately, it's not possible. Das geht leider nicht.\n",
      "[129.88s - 131.00s] UNKNOWN_SPEAKER: Die\n",
      "[131.00s - 133.74s] SPEAKER_00: Wiedpunkt kommen zlesch Deutsch-Treiner.\n",
      "\n",
      "--- End of Transcript ---\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "# Do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load environment variables from .env file\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "AUDIO_FILE = \"Deutschtrainer-002-Im-Gespraech-Englisch.mp3\" # <--- Replace with your audio file path\n",
    "WHISPER_MODEL = \"tiny\" # Choose 'tiny', 'base', 'small' for \"Whisper Mini\"\n",
    "PYANNOTE_PIPELINE = \"pyannote/speaker-diarization-3.1\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') # <--- Replace with your token or ensure logged in via CLI\n",
    "\n",
    "# --- Check for GPU ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. Load Diarization Pipeline ---\n",
    "print(\"Loading diarization pipeline...\")\n",
    "try:\n",
    "    # If using token directly:\n",
    "    # pipeline = Pipeline.from_pretrained(PYANNOTE_PIPELINE, use_auth_token=HF_TOKEN)\n",
    "    # If logged in via CLI:\n",
    "    pipeline = Pipeline.from_pretrained(PYANNOTE_PIPELINE, use_auth_token=HF_TOKEN)\n",
    "    pipeline.to(torch.device(DEVICE))\n",
    "    print(\"Diarization pipeline loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading diarization pipeline: {e}\")\n",
    "    print(\"Please ensure you have accepted user conditions on Hugging Face Hub and have a valid token.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Perform Speaker Diarization ---\n",
    "print(\"Performing speaker diarization...\")\n",
    "try:\n",
    "    diarization = pipeline(AUDIO_FILE, num_speakers=None) # Let pyannote detect number of speakers\n",
    "    # Or specify num_speakers if known: diarization = pipeline(AUDIO_FILE, num_speakers=2)\n",
    "    print(\"Diarization complete.\")\n",
    "\n",
    "    # Convert pyannote diarization to a list of speaker segments for easier lookup\n",
    "    speaker_segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speaker_segments.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    print(f\"Found {len(speaker_segments)} speaker turns.\")\n",
    "    if not speaker_segments:\n",
    "        print(\"Warning: No speaker segments found by pyannote.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during diarization: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Load Whisper Model and Transcribe ---\n",
    "print(f\"Loading Whisper model '{WHISPER_MODEL}'...\")\n",
    "whisper_model = whisper.load_model(WHISPER_MODEL, device=DEVICE)\n",
    "print(\"Whisper model loaded.\")\n",
    "\n",
    "print(\"Transcribing audio with word timestamps...\")\n",
    "try:\n",
    "    # Set word_timestamps=True\n",
    "    options = whisper.DecodingOptions(fp16 = torch.cuda.is_available()) # fp16 only works on CUDA\n",
    "    result = whisper_model.transcribe(AUDIO_FILE, word_timestamps=True, **vars(options))\n",
    "    print(\"Transcription complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during transcription: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. Align Transcription with Diarization ---\n",
    "print(\"Aligning transcription with speaker segments...\")\n",
    "\n",
    "# Function to find the speaker for a given timestamp\n",
    "def get_speaker_for_timestamp(timestamp, segments):\n",
    "    for segment in segments:\n",
    "        if segment[\"start\"] <= timestamp < segment[\"end\"]:\n",
    "            return segment[\"speaker\"]\n",
    "    # Handle edge cases or words outside detected segments (assign to nearest? or mark unknown?)\n",
    "    # Simple approach: return None or a default label if no segment matches\n",
    "    # More robust: find the *closest* segment (might be needed for gaps)\n",
    "    return \"UNKNOWN_SPEAKER\" # Or handle this case as needed\n",
    "\n",
    "aligned_transcript = []\n",
    "# Process Whisper results, which can have multiple segments\n",
    "if 'segments' in result:\n",
    "    for segment in result['segments']:\n",
    "        if 'words' in segment:\n",
    "            for word_info in segment['words']:\n",
    "                word_start = word_info['start']\n",
    "                word_end = word_info['end']\n",
    "                word_text = word_info['word']\n",
    "\n",
    "                # Use the middle of the word time to find the speaker\n",
    "                word_mid_time = word_start + (word_end - word_start) / 2\n",
    "\n",
    "                # Find the speaker segment this word belongs to\n",
    "                speaker_label = get_speaker_for_timestamp(word_mid_time, speaker_segments)\n",
    "\n",
    "                aligned_transcript.append({\n",
    "                    \"start\": word_start,\n",
    "                    \"end\": word_end,\n",
    "                    \"word\": word_text,\n",
    "                    \"speaker\": speaker_label\n",
    "                })\n",
    "        else:\n",
    "             print(\"Warning: Segment found with no 'words' key. Check Whisper output structure.\")\n",
    "\n",
    "else:\n",
    "    print(\"Warning: No 'segments' key found in Whisper result. Check Whisper output structure.\")\n",
    "\n",
    "\n",
    "print(\"Alignment complete.\")\n",
    "\n",
    "# --- 5. Format and Print Output ---\n",
    "print(\"\\n--- Speaker-Aligned Transcript ---\")\n",
    "\n",
    "if not aligned_transcript:\n",
    "    print(\"No words found to align.\")\n",
    "else:\n",
    "    current_speaker = aligned_transcript[0]['speaker']\n",
    "    current_segment_start = aligned_transcript[0]['start']\n",
    "    current_text = \"\"\n",
    "\n",
    "    for i, word_data in enumerate(aligned_transcript):\n",
    "        speaker = word_data['speaker']\n",
    "        word = word_data['word']\n",
    "        end_time = word_data['end']\n",
    "\n",
    "        if speaker == current_speaker:\n",
    "            current_text += word\n",
    "        else:\n",
    "            # Speaker changed, print previous segment\n",
    "            print(f\"[{current_segment_start:.2f}s - {last_end_time:.2f}s] {current_speaker}: {current_text.strip()}\")\n",
    "            # Start new segment\n",
    "            current_speaker = speaker\n",
    "            current_segment_start = word_data['start']\n",
    "            current_text = word\n",
    "\n",
    "        last_end_time = end_time # Keep track of the end time of the last word processed\n",
    "\n",
    "        # Print the last segment after the loop finishes\n",
    "        if i == len(aligned_transcript) - 1:\n",
    "             print(f\"[{current_segment_start:.2f}s - {last_end_time:.2f}s] {current_speaker}: {current_text.strip()}\")\n",
    "\n",
    "print(\"\\n--- End of Transcript ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66f34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
