{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631aa33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\afi89\\Documents\\Github\\DiCoW\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from speechbox import ASRDiarizationPipeline # Only import this\n",
    "# from datasets import load_dataset # Not needed if loading local files manually\n",
    "from datasets import Dataset\n",
    "import os\n",
    "# from pyannote.audio import Pipeline # No longer needed here\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv() \n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# instantiate the pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=HF_TOKEN)\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "diarization = pipeline(\"transcribing_1.mp3\")\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "with open(\"audio.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9551b3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading diarization pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization pipeline loaded.\n",
      "Performing speaker diarization...\n",
      "Diarization complete.\n",
      "Found 294 speaker turns.\n",
      "Loading Whisper model 'tiny'...\n",
      "Whisper model loaded.\n",
      "Transcribing audio with word timestamps...\n",
      "Transcription complete.\n",
      "Aligning transcription with speaker segments...\n",
      "Alignment complete.\n",
      "\n",
      "--- Speaker-Aligned Transcript ---\n",
      "[0.00s - 17.54s] SPEAKER_00: Okay, so let's jump right in. Today, we're diving into something that I think is really like on everyone's mind these days. How do we wrap our heads around these really advanced AI systems, especially the ones we keep hearing about the large language models or LLMs?\n",
      "[17.62s - 18.42s] SPEAKER_01: Right, right.\n",
      "[18.52s - 20.46s] SPEAKER_00: It can feel so, so technical.\n",
      "[20.66s - 21.12s] SPEAKER_01: Absolutely.\n",
      "[21.70s - 23.56s] SPEAKER_00: But you know, what if we could take a shortcut?\n",
      "[24.30s - 25.16s] SPEAKER_01: Interesting. What if\n",
      "[25.16s - 26.00s] SPEAKER_00: we thought about it\n",
      "[26.00s - 27.14s] UNKNOWN_SPEAKER: like,\n",
      "[27.14s - 29.22s] SPEAKER_00: like, psychologist, study the human mind?\n",
      "[29.22s - 31.00s] SPEAKER_01: Okay, I like where you're going with those. They\n",
      "[31.00s - 36.18s] SPEAKER_00: have all these tests, all these theories, right? Right. So could we apply those same ideas to AI\n",
      "[36.18s - 37.12s] SPEAKER_01: to\n",
      "[37.12s - 38.80s] SPEAKER_00: see what's really going on inside?\n",
      "[39.12s - 39.62s] SPEAKER_01: Fascinating.\n",
      "[39.64s - 42.78s] SPEAKER_00: That's the whole idea behind what summer calling machine psychology.\n",
      "[43.04s - 45.00s] SPEAKER_01: Ah, that's really intriguing. It's\n",
      "[45.00s - 49.50s] SPEAKER_00: like a new lens to understand this whole AI world that's changing so fast.\n",
      "[49.52s - 50.06s] SPEAKER_01: I'm with you.\n",
      "[50.26s - 56.26s] SPEAKER_00: And the paper we're looking at today, this machine psychology paper by Hagen Doref and their team, really lays it all out.\n",
      "[56.32s - 57.66s] SPEAKER_01: That's a great starting point.\n",
      "[57.66s - 58.30s] SPEAKER_00: So\n",
      "[58.30s - 59.04s] UNKNOWN_SPEAKER: I\n",
      "[59.04s - 64.70s] SPEAKER_00: guess first things first. Why even go down this road? Why is machine psychology suddenly a\n",
      "[64.70s - 65.16s] UNKNOWN_SPEAKER: thing?\n",
      "[65.46s - 69.24s] SPEAKER_01: Well, because the old ways of looking at AI, they're just not cutting it anymore.\n",
      "[69.64s - 71.22s] SPEAKER_00: Okay, can you unpack that a bit? What do you mean?\n",
      "[71.44s - 73.86s] SPEAKER_01: For a long time, we were all about performance, you know?\n",
      "[74.12s - 77.62s] SPEAKER_00: Right. Like, can this AI translate this text correctly?\n",
      "[77.92s - 84.84s] SPEAKER_01: Exactly. Or winnet chess or, you know, solve a math problem. It was all about benchmarks and scores.\n",
      "[85.26s - 85.74s] SPEAKER_00: Yeah, yeah.\n",
      "[85.74s - 88.40s] SPEAKER_01: But LLMs, they've changed the game.\n",
      "[88.72s - 90.76s] SPEAKER_00: Yeah, I mean, they're doing some pretty wild stuff.\n",
      "[90.96s - 96.04s] SPEAKER_01: Exactly. There's so much more sophisticated now that those simple scores, they don't tell the whole story.\n",
      "[96.48s - 101.30s] SPEAKER_00: So like, a high score doesn't necessarily mean we understand how the AI is actually thinking.\n",
      "[101.68s - 105.40s] SPEAKER_01: It's like a match and judging a friend just by how well they do on a trivia night.\n",
      "[105.40s - 105.84s] SPEAKER_00: Ah-huh,\n",
      "[106.06s - 112.42s] SPEAKER_01: right. You'd miss all the nuances of their personality, their thought process, their, you know, unique way of approaching things. Totally.\n",
      "[112.42s - 113.12s] UNKNOWN_SPEAKER: And\n",
      "[113.12s - 119.30s] SPEAKER_01: LLMs, they started showing these emergent abilities. Things they weren't specifically trained to do.\n",
      "[119.46s - 121.10s] SPEAKER_00: Right, like learning from just a few examples.\n",
      "[121.58s - 126.68s] SPEAKER_01: Exactly. That 2020 paper by Brown and others, it really highlighted that.\n",
      "[126.68s - 127.54s] SPEAKER_00: It was kind of mind blowing.\n",
      "[127.86s - 132.80s] SPEAKER_01: Totally. And it made those old performance checklists seem, well, pretty inadequate.\n",
      "[133.18s - 135.74s] SPEAKER_00: So, okay, what happened? What did researchers do?\n",
      "[135.92s - 142.06s] SPEAKER_01: They took a page from human psychology. Oh, interest. They started developing these test-only benchmarks.\n",
      "[142.06s - 142.58s] UNKNOWN_SPEAKER: Okay,\n",
      "[142.76s - 142.76s] SPEAKER_00: with\n",
      "[142.76s - 143.38s] UNKNOWN_SPEAKER: that?\n",
      "[143.64s - 157.70s] SPEAKER_01: Imagine giving an AI a personality quiz, or an IQ test. Well, really. But the goal isn't just a score. Right. It's about revealing how the LLM behaves. The process, the strategies, the underlying thinking.\n",
      "[157.90s - 161.70s] SPEAKER_00: Ah, so we're not just looking at the what anymore. We're looking at the how.\n",
      "[161.98s - 163.50s] SPEAKER_01: Precisely, that's the big shift.\n",
      "[163.64s - 166.42s] SPEAKER_00: Okay, so I see why machines psychology is emerging.\n",
      "[166.64s - 166.82s] SPEAKER_01: Yeah.\n",
      "[167.30s - 169.80s] SPEAKER_00: But why study LLM behavior directly? The\n",
      "[169.80s - 170.10s] SPEAKER_01: question.\n",
      "[170.38s - 172.88s] SPEAKER_00: Why not just, you know, stick to analyzing their code.\n",
      "[172.94s - 173.00s] SPEAKER_01: Mm.\n",
      "[173.54s - 176.30s] SPEAKER_00: We're comparing them to, you know, how the human brain works.\n",
      "[176.56s - 180.86s] SPEAKER_01: So analyzing the inner workings, like the code, that's mechanistic interpretability.\n",
      "[181.00s - 181.28s] SPEAKER_00: Okay.\n",
      "[181.72s - 191.56s] SPEAKER_01: And drawing comparisons to the brain, that's neuroscience inspired. Good. Both valuable, but studying behavior directly, it gives us a different kind of insight.\n",
      "[191.84s - 192.38s] SPEAKER_00: And what way?\n",
      "[192.82s - 218.16s] SPEAKER_01: Well, first, the behavior we see, the inputs and outputs, that's what happens at the human AI interface. Yeah. It's what you as a user actually experience. So it has a media relevance. Big sense. Second, these models are getting so complex, so massive. That predicting their behavior, just by looking at their internal mechanisms, their code, it's becoming incredibly difficult.\n",
      "[218.20s - 218.50s] SPEAKER_00: Oh.\n",
      "[219.08s - 221.26s] SPEAKER_01: Research by Kaplan and Wei, they've shown this.\n",
      "[221.42s - 226.76s] SPEAKER_00: So it's kind of like, we can't always tell what a car will do, just by staring at the engine.\n",
      "[226.98s - 228.10s] SPEAKER_01: That's a great analogy. We\n",
      "[228.10s - 229.92s] SPEAKER_00: need to see it in action on the road.\n",
      "[230.04s - 230.70s] SPEAKER_01: Exactly. See\n",
      "[230.70s - 231.32s] SPEAKER_00: how it behaves.\n",
      "[231.56s - 239.32s] SPEAKER_01: Yes. And with AI, understanding what it does, how it behaves, that's becoming more practical, more scalable, than just trying to peek under the hood.\n",
      "[239.36s - 239.66s] SPEAKER_00: I see.\n",
      "[239.68s - 242.92s] SPEAKER_01: Especially with these cutting-edge models, the ones that are closed source. Right.\n",
      "[243.08s - 247.68s] SPEAKER_00: The ones where the companies don't share all the details of how they work. Okay. So bottom\n",
      "[247.68s - 248.22s] UNKNOWN_SPEAKER: line,\n",
      "[248.84s - 251.36s] SPEAKER_00: we need new tools to understand these new AI's.\n",
      "[251.38s - 252.58s] SPEAKER_01: That's a great way to put it.\n",
      "[252.58s - 255.00s] SPEAKER_00: And machine psychology is offering some of those tools. It\n",
      "[255.00s - 255.46s] SPEAKER_01: is.\n",
      "[255.54s - 260.96s] SPEAKER_00: So how does it actually work? How do we take ideas from human psychology and apply them to machines?\n",
      "[261.26s - 269.80s] SPEAKER_01: Well, we adapt. We take the theoretical frameworks, the experimental designs that have been developed in human psychology, cognitive science, behavioral sciences. Right.\n",
      "[269.94s - 270.82s] SPEAKER_00: Decky, it's a research.\n",
      "[270.92s - 274.18s] SPEAKER_01: Exactly. And we apply them to LLMs.\n",
      "[274.48s - 277.54s] SPEAKER_00: So like, think back to those foundational studies. You know?\n",
      "[277.74s - 278.10s] SPEAKER_01: Yeah.\n",
      "[278.10s - 280.30s] SPEAKER_00: Like the work by Edwards or Festing your Incax.\n",
      "[280.50s - 281.80s] SPEAKER_01: Those are great examples. They\n",
      "[281.80s - 285.10s] SPEAKER_00: were trying to understand why humans behave the way they do.\n",
      "[285.14s - 285.44s] SPEAKER_01: Right.\n",
      "[285.44s - 288.42s] SPEAKER_00: And now we're taking those methods and applying them to AI.\n",
      "[289.02s - 297.58s] SPEAKER_01: Absolutely. It's about designing experiments where we can link the AI's behavior to its potential internal representations, capabilities, or thought processes.\n",
      "[298.70s - 302.92s] SPEAKER_00: Okay. So like, if we think at LLM might be able to reason logically.\n",
      "[303.50s - 304.06s] UNKNOWN_SPEAKER: We\n",
      "[304.06s - 306.66s] SPEAKER_00: design a task where that reasoning would lead\n",
      "[306.66s - 307.50s] UNKNOWN_SPEAKER: to\n",
      "[307.50s - 309.66s] SPEAKER_00: a specific predictable output.\n",
      "[309.90s - 314.50s] SPEAKER_01: That's the idea. A kind of scientific if then test for the AI's abilities.\n",
      "[314.68s - 318.86s] SPEAKER_00: Right. If the agent has X, we expect to see why.\n",
      "[318.88s - 320.66s] SPEAKER_01: Exactly. Otherwise, we should see Z.\n",
      "[320.84s - 321.52s] SPEAKER_00: Okay. I'm following.\n",
      "[321.84s - 326.10s] SPEAKER_01: And just like in human psychology, having good experimental controls is absolutely vital.\n",
      "[326.46s - 328.24s] SPEAKER_00: Right. So we know we're not fooling ourselves.\n",
      "[328.48s - 334.04s] SPEAKER_01: We need to be sure that the behavior we're seeing truly reflects the ability we're interested in.\n",
      "[334.06s - 334.82s] SPEAKER_00: Makes sense.\n",
      "[335.12s - 338.12s] SPEAKER_01: Boring back in the day, he really emphasized this.\n",
      "[338.14s - 339.72s] SPEAKER_00: So careful design is key.\n",
      "[340.02s - 345.04s] SPEAKER_01: Absolutely. Another thing to keep in mind is the performance competence distinction.\n",
      "[345.48s - 346.12s] SPEAKER_00: Okay. What can\n",
      "[346.12s - 352.56s] SPEAKER_01: I? Chomsky introduced it in linguistics. And Firestone and Lampion have talked about it with AI.\n",
      "[352.84s - 358.18s] SPEAKER_00: Basically, just because an LLM makes a mistake, it doesn't automatically mean it lacks the underlying ability.\n",
      "[358.28s - 358.88s] SPEAKER_01: Precisely.\n",
      "[359.10s - 359.30s] SPEAKER_00: Right.\n",
      "[359.54s - 363.92s] SPEAKER_01: It's performance in that moment might not reflect its true competence.\n",
      "[364.54s - 366.02s] SPEAKER_00: That's an important nuance.\n",
      "[366.24s - 368.18s] SPEAKER_01: It is. We tend to do that with humans too, right?\n",
      "[368.18s - 368.56s] SPEAKER_00: Yeah.\n",
      "[369.10s - 370.92s] SPEAKER_01: Yeah. Based on one interaction.\n",
      "[371.38s - 372.36s] SPEAKER_00: Right. Or one bed day.\n",
      "[372.36s - 373.16s] SPEAKER_01: It's exact.\n",
      "[373.26s - 380.52s] SPEAKER_00: Okay. So let's get concrete. Sure. The paper talks about heuristics and biases. A big area in human decision making.\n",
      "[380.60s - 380.94s] SPEAKER_01: Right.\n",
      "[381.24s - 382.18s] SPEAKER_00: Can you explain that a bit?\n",
      "[382.76s - 388.64s] SPEAKER_01: So heuristics are mental shortcuts. We use them all the time to make quick decisions.\n",
      "[388.80s - 389.00s] SPEAKER_00: Thanks,\n",
      "[389.04s - 396.50s] SPEAKER_01: sense. Geigerens are Geismar, Traversky, Konoman. Their work has been foundational in this area.\n",
      "[396.62s - 399.50s] SPEAKER_00: Right. They're like the giants of behavioral economics and psychology.\n",
      "[399.70s - 407.98s] SPEAKER_01: They are. And they showed how these shortcuts, these heuristics, can be efficient, but they can also lead to systematic errors.\n",
      "[408.44s - 410.90s] SPEAKER_00: Ah. So like we're biased in predictable ways.\n",
      "[410.96s - 411.48s] SPEAKER_01: Exactly.\n",
      "[411.80s - 412.28s] SPEAKER_00: Fascinating.\n",
      "[412.50s - 424.14s] SPEAKER_01: And what's really interesting is that early machine psychology research, like by bins and shults, they found that earlier LLMs, like GPT-3, they actually exhibited some similar cognitive biases to humans.\n",
      "[424.30s - 427.58s] SPEAKER_00: No way. So like the same mental quirks that trip us up.\n",
      "[427.58s - 438.82s] SPEAKER_01: It seems so. Things like framing effects where how information is presented influences our judgment. Right. Right. Or the availability heuristic, where we rely to heavily on information that's easy to recall.\n",
      "[439.18s - 442.32s] SPEAKER_00: Ah, yeah. Those seem to affect LLM responses, too.\n",
      "[442.46s - 448.52s] SPEAKER_01: Wow. So these sophisticated machines are making some of the same mental missteps that we do.\n",
      "[448.68s - 451.62s] SPEAKER_00: It appears so. At least with those earlier models.\n",
      "[452.02s - 454.40s] SPEAKER_01: Okay. So what about the latest and greatest LLMs?\n",
      "[455.06s - 464.78s] SPEAKER_00: Well, here's where things get really intriguing. Research by Chen and Hagendorf suggests that those classic biases they seem to have largely disappeared.\n",
      "[465.64s - 471.94s] SPEAKER_01: What? Really? So the AI is getting smarter and less biased than us. Well,\n",
      "[471.96s - 473.12s] SPEAKER_00: it's not quite that simple.\n",
      "[473.14s - 473.90s] SPEAKER_01: Okay. What's going on\n",
      "[473.90s - 486.86s] SPEAKER_00: there? There are a couple of hypotheses. Let's hear. One is that these newer models, they have such increased reasoning abilities. Okay. That they can analyze problems more thoroughly and overcome those simpler intuitive biases.\n",
      "[487.18s - 491.20s] SPEAKER_01: So the tests that used to trick the AI, they're not challenging enough anymore.\n",
      "[491.44s - 494.42s] SPEAKER_00: It's possible. Another possibility is data leakage.\n",
      "[494.72s - 495.36s] SPEAKER_01: Oh, interesting.\n",
      "[495.90s - 498.82s] SPEAKER_00: The chance that the massive data sets these models are trained on.\n",
      "[498.82s - 507.58s] SPEAKER_01: Right. It's like huge amounts of text and code. Exactly. It's possible that they've inadvertently included information about those very psychological tests.\n",
      "[507.68s - 513.98s] SPEAKER_00: So the AI might be learning the correct answers without actually overcoming the underlying bias.\n",
      "[513.98s - 516.02s] SPEAKER_01: That's a concern researchers are looking into.\n",
      "[516.10s - 517.22s] SPEAKER_00: So it's kind of a mystery still.\n",
      "[517.40s - 518.40s] SPEAKER_01: Some extent, yes.\n",
      "[518.64s - 530.14s] SPEAKER_00: But even if those classic biases are fading, it seems that framing effects how a problem is worded that still influences LLMs. Right. Absolutely. And that work by desk, Gupta, and Shubrit.\n",
      "[530.30s - 540.38s] SPEAKER_01: Exactly. And it mirrors what we know about human psychology. Early findings by Chiang and Holyoke, Tversky and Kahneman, they showed how powerful framing can be.\n",
      "[540.52s - 546.88s] SPEAKER_00: Right. How you ask a question really matters. It does. Okay. So framing still matters. What else is the field looking\n",
      "[546.88s - 547.36s] UNKNOWN_SPEAKER: at?\n",
      "[547.48s - 550.38s] SPEAKER_01: Well, there's this idea of ecological rationality.\n",
      "[550.44s - 550.96s] SPEAKER_00: Not to do\n",
      "[550.96s - 551.22s] UNKNOWN_SPEAKER: one.\n",
      "[551.26s - 558.60s] SPEAKER_01: Todd and Jigerenz are explored this idea. It's the notion that human reasoning is shaped by the specific environments we operate in.\n",
      "[558.92s - 561.58s] SPEAKER_00: Okay. So like our surroundings influence how we think.\n",
      "[561.78s - 569.32s] SPEAKER_01: It away, yes. And researchers like Chan and McCoy, they're investigating how the characteristics of the data LLMs are trained on.\n",
      "[569.36s - 570.62s] SPEAKER_00: Right. Those massive data sets.\n",
      "[570.70s - 576.24s] SPEAKER_01: Exactly. They're looking at how that data shapes the AI's behavior and reasoning processes.\n",
      "[576.48s - 582.42s] SPEAKER_00: Fascinating. So let's move to another area that's, I think, really captivating. So yeah. Social interactions.\n",
      "[582.86s - 583.64s] SPEAKER_01: Okay. Yes.\n",
      "[584.34s - 585.96s] SPEAKER_00: Specifically theory of mind.\n",
      "[587.98s - 590.64s] SPEAKER_01: A foundational concept in developmental psychology.\n",
      "[590.82s - 591.88s] SPEAKER_00: Can you give us a quick rundown?\n",
      "[592.38s - 601.46s] SPEAKER_01: It's the ability to understand that other people have their own thoughts, beliefs, desires, intentions, and that those can be different from your own.\n",
      "[601.60s - 603.66s] SPEAKER_00: It's like stepping into someone else's shoes.\n",
      "[603.96s - 607.52s] SPEAKER_01: It is. It's crucial for navigating social situations effectively.\n",
      "[608.12s - 609.82s] SPEAKER_00: So how do we study that in AI?\n",
      "[610.18s - 615.68s] SPEAKER_01: Researchers are adapting classic theory of mind tasks, like those developed by whimmer and purner.\n",
      "[615.70s - 615.96s] SPEAKER_00: Okay.\n",
      "[616.32s - 619.76s] SPEAKER_01: They're seeing if LLMs can demonstrate that same kind of understanding.\n",
      "[620.06s - 620.70s] SPEAKER_00: And what's the verdict?\n",
      "[620.94s - 628.62s] SPEAKER_01: Well, early models, they struggled. Yeah. Studies with models like GPT-3, like that worked by Sapping colleagues. They showed limitations.\n",
      "[629.02s - 629.44s] SPEAKER_00: Interesting.\n",
      "[629.70s - 632.88s] SPEAKER_01: But more recent research, it's painting a different picture.\n",
      "[633.12s - 633.20s] SPEAKER_00: Oh.\n",
      "[633.54s - 638.96s] SPEAKER_01: Groups like Stri Chan, Hulterman and Demeter, Mugatum and Honey.\n",
      "[639.28s - 640.48s] SPEAKER_00: Yeah. I've seen some of that work.\n",
      "[640.62s - 646.34s] SPEAKER_01: They're suggesting that newer LLMs are showing much more sophisticated theory of mind capability.\n",
      "[646.58s - 647.74s] SPEAKER_00: Wow. That's a big jump.\n",
      "[647.98s - 653.18s] SPEAKER_01: It is. In some cases, their performances even being compared to that of children.\n",
      "[653.46s - 654.30s] SPEAKER_00: Seriously. Dream\n",
      "[654.30s - 657.76s] SPEAKER_01: and colleagues, they've done some fascinating work in that area.\n",
      "[658.26s - 661.22s] SPEAKER_00: So AI is catching up to human kids.\n",
      "[661.62s - 665.76s] SPEAKER_01: Well, in some very specific ways, it seems so. And the research is expanding.\n",
      "[665.90s - 666.10s] SPEAKER_00: Right.\n",
      "[666.22s - 672.24s] SPEAKER_01: There's work on higher order theory of mind, understanding what others think about what others are thinking.\n",
      "[672.32s - 673.48s] SPEAKER_00: Whoa, that's good and meta.\n",
      "[673.90s - 679.34s] SPEAKER_01: It is. Research by Street is exploring that. Well. And Omen has been looking at how robust these tests are.\n",
      "[679.46s - 682.18s] SPEAKER_00: Right. Making sure they're actually measuring what we think they're measuring.\n",
      "[682.34s - 694.34s] SPEAKER_01: Exactly. And here's what I think is particularly fascinating. Recent work, like by Hagen, Dwarf in 2024, is suggesting that LLMs might be able to induce false beliefs in other artificial agents.\n",
      "[694.70s - 700.00s] SPEAKER_00: Whoa, hold on. So like, making another AI believe something that's not true.\n",
      "[700.36s - 701.56s] SPEAKER_01: That's what the research is suggesting.\n",
      "[701.60s - 702.90s] SPEAKER_00: That's kind of mind blowing.\n",
      "[703.00s - 708.72s] SPEAKER_01: It is. And Lou and colleagues are looking at how LLMs handle communicative values like honesty.\n",
      "[709.36s - 712.38s] SPEAKER_00: So like, can they lie? Can they be manipulative?\n",
      "[712.38s - 713.68s] SPEAKER_01: That's what we're trying to understand.\n",
      "[713.70s - 715.88s] SPEAKER_00: Wow, this is all getting really deep, really fast.\n",
      "[716.00s - 721.88s] SPEAKER_01: It is. And it has huge implications as these AI systems become more integrated into our lives.\n",
      "[721.94s - 728.64s] SPEAKER_00: Right. Right. If we're going to trust these AI, we need to understand how they think, how they behave, especially in social situations.\n",
      "[729.12s - 729.48s] SPEAKER_01: Absolutely.\n",
      "[729.96s - 731.78s] SPEAKER_00: Okay. So I think we've covered a lot of ground here.\n",
      "[731.92s - 732.38s] SPEAKER_01: We have.\n",
      "[732.40s - 733.54s] SPEAKER_00: What's the big takeaway for you?\n",
      "[733.76s - 752.02s] SPEAKER_01: Well, for me, it's that the shift from just looking at AI performance to analyzing their actual behavior. It's a game changer. Right. And by using the knowledge we've gained from studying human minds, we can get a much deeper, more nuanced understanding of what machine intelligence really is.\n",
      "[752.20s - 753.54s] SPEAKER_00: It's not just about scores anymore.\n",
      "[753.90s - 755.00s] SPEAKER_01: It's about understanding\n",
      "[755.00s - 755.82s] UNKNOWN_SPEAKER: their\n",
      "[755.82s - 759.20s] SPEAKER_01: reasoning, their biases, their social abilities. It's\n",
      "[759.20s - 763.16s] SPEAKER_00: about seeing them as in a way more like us.\n",
      "[763.34s - 764.44s] SPEAKER_01: In some ways, yes.\n",
      "[764.74s - 768.34s] SPEAKER_00: And for everyone listening, this isn't just some abstract academic exercise.\n",
      "[768.48s - 768.76s] SPEAKER_01: Right.\n",
      "[768.86s - 775.34s] SPEAKER_00: As LLMs become more and more a part of our daily lives. Understanding their behavior is going to be absolutely crucial. We've had\n",
      "[775.34s - 776.06s] SPEAKER_01: to everything.\n",
      "[776.28s - 782.68s] SPEAKER_00: How we interact with them, how we design them, how we ensure their safe, reliable, and aligned with our values.\n",
      "[782.76s - 783.96s] SPEAKER_01: It's a huge responsibility.\n",
      "[784.36s - 797.30s] SPEAKER_00: It is. So I guess my final thought for everyone out there is this. As AI continues to evolve, to become even more sophisticated. What other aspects of human psychology might we need to explore?\n",
      "[797.56s - 798.88s] SPEAKER_01: That's the big question.\n",
      "[798.96s - 807.68s] SPEAKER_00: What other tools from our understanding of the human mind might help us navigate this new world of incredibly powerful AI.\n",
      "[807.72s - 809.02s] SPEAKER_01: It's a fascinating challenge.\n",
      "[809.46s - 816.08s] SPEAKER_00: It is. And I think it's one that we're all going to be grappling with for a long time to come. Thanks for diving in with us today.\n",
      "[816.18s - 816.88s] SPEAKER_01: It's been a pleasure.\n",
      "\n",
      "--- End of Transcript ---\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "# Do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load environment variables from .env file\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "AUDIO_FILE = \"MP.wav\" # <--- Replace with your audio file path\n",
    "WHISPER_MODEL = \"tiny\" # Choose 'tiny', 'base', 'small' for \"Whisper Mini\"\n",
    "PYANNOTE_PIPELINE = \"pyannote/speaker-diarization-3.1\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') # <--- Replace with your token or ensure logged in via CLI\n",
    "\n",
    "# --- Check for GPU ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. Load Diarization Pipeline ---\n",
    "print(\"Loading diarization pipeline...\")\n",
    "try:\n",
    "    # If using token directly:\n",
    "    # pipeline = Pipeline.from_pretrained(PYANNOTE_PIPELINE, use_auth_token=HF_TOKEN)\n",
    "    # If logged in via CLI:\n",
    "    pipeline = Pipeline.from_pretrained(PYANNOTE_PIPELINE, use_auth_token=HF_TOKEN)\n",
    "    pipeline.to(torch.device(DEVICE))\n",
    "    print(\"Diarization pipeline loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading diarization pipeline: {e}\")\n",
    "    print(\"Please ensure you have accepted user conditions on Hugging Face Hub and have a valid token.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Perform Speaker Diarization ---\n",
    "print(\"Performing speaker diarization...\")\n",
    "try:\n",
    "    diarization = pipeline(AUDIO_FILE, num_speakers=None) # Let pyannote detect number of speakers\n",
    "    # Or specify num_speakers if known: diarization = pipeline(AUDIO_FILE, num_speakers=2)\n",
    "    print(\"Diarization complete.\")\n",
    "\n",
    "    # Convert pyannote diarization to a list of speaker segments for easier lookup\n",
    "    speaker_segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speaker_segments.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    print(f\"Found {len(speaker_segments)} speaker turns.\")\n",
    "    if not speaker_segments:\n",
    "        print(\"Warning: No speaker segments found by pyannote.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during diarization: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Load Whisper Model and Transcribe ---\n",
    "print(f\"Loading Whisper model '{WHISPER_MODEL}'...\")\n",
    "whisper_model = whisper.load_model(WHISPER_MODEL, device=DEVICE)\n",
    "print(\"Whisper model loaded.\")\n",
    "\n",
    "print(\"Transcribing audio with word timestamps...\")\n",
    "try:\n",
    "    # Set word_timestamps=True\n",
    "    options = whisper.DecodingOptions(fp16 = torch.cuda.is_available()) # fp16 only works on CUDA\n",
    "    result = whisper_model.transcribe(AUDIO_FILE, word_timestamps=True, **vars(options))\n",
    "    print(\"Transcription complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during transcription: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. Align Transcription with Diarization ---\n",
    "print(\"Aligning transcription with speaker segments...\")\n",
    "\n",
    "# Function to find the speaker for a given timestamp\n",
    "def get_speaker_for_timestamp(timestamp, segments):\n",
    "    for segment in segments:\n",
    "        if segment[\"start\"] <= timestamp < segment[\"end\"]:\n",
    "            return segment[\"speaker\"]\n",
    "    # Handle edge cases or words outside detected segments (assign to nearest? or mark unknown?)\n",
    "    # Simple approach: return None or a default label if no segment matches\n",
    "    # More robust: find the *closest* segment (might be needed for gaps)\n",
    "    return \"UNKNOWN_SPEAKER\" # Or handle this case as needed\n",
    "\n",
    "aligned_transcript = []\n",
    "# Process Whisper results, which can have multiple segments\n",
    "if 'segments' in result:\n",
    "    for segment in result['segments']:\n",
    "        if 'words' in segment:\n",
    "            for word_info in segment['words']:\n",
    "                word_start = word_info['start']\n",
    "                word_end = word_info['end']\n",
    "                word_text = word_info['word']\n",
    "\n",
    "                # Use the middle of the word time to find the speaker\n",
    "                word_mid_time = word_start + (word_end - word_start) / 2\n",
    "\n",
    "                # Find the speaker segment this word belongs to\n",
    "                speaker_label = get_speaker_for_timestamp(word_mid_time, speaker_segments)\n",
    "\n",
    "                aligned_transcript.append({\n",
    "                    \"start\": word_start,\n",
    "                    \"end\": word_end,\n",
    "                    \"word\": word_text,\n",
    "                    \"speaker\": speaker_label\n",
    "                })\n",
    "        else:\n",
    "             print(\"Warning: Segment found with no 'words' key. Check Whisper output structure.\")\n",
    "\n",
    "else:\n",
    "    print(\"Warning: No 'segments' key found in Whisper result. Check Whisper output structure.\")\n",
    "\n",
    "\n",
    "print(\"Alignment complete.\")\n",
    "\n",
    "# --- 5. Format and Print Output ---\n",
    "print(\"\\n--- Speaker-Aligned Transcript ---\")\n",
    "\n",
    "if not aligned_transcript:\n",
    "    print(\"No words found to align.\")\n",
    "else:\n",
    "    current_speaker = aligned_transcript[0]['speaker']\n",
    "    current_segment_start = aligned_transcript[0]['start']\n",
    "    current_text = \"\"\n",
    "\n",
    "    for i, word_data in enumerate(aligned_transcript):\n",
    "        speaker = word_data['speaker']\n",
    "        word = word_data['word']\n",
    "        end_time = word_data['end']\n",
    "\n",
    "        if speaker == current_speaker:\n",
    "            current_text += word\n",
    "        else:\n",
    "            # Speaker changed, print previous segment\n",
    "            print(f\"[{current_segment_start:.2f}s - {last_end_time:.2f}s] {current_speaker}: {current_text.strip()}\")\n",
    "            # Start new segment\n",
    "            current_speaker = speaker\n",
    "            current_segment_start = word_data['start']\n",
    "            current_text = word\n",
    "\n",
    "        last_end_time = end_time # Keep track of the end time of the last word processed\n",
    "\n",
    "        # Print the last segment after the loop finishes\n",
    "        if i == len(aligned_transcript) - 1:\n",
    "             print(f\"[{current_segment_start:.2f}s - {last_end_time:.2f}s] {current_speaker}: {current_text.strip()}\")\n",
    "\n",
    "print(\"\\n--- End of Transcript ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66f34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
